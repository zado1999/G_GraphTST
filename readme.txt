G-GraphTST is a novel Global-Embedding Graph-Encoder Time Series Transformer designed for multivariate long-term photovoltaic power forecasting. Our model addresses the limitations of existing methods by effectively modeling spatiotemporal correlations at different perceptual levels and incorporating interactive learning between spatial and temporal dependencies.
ğŸŒŸ Key Features

Global Embedding Mechanism: Captures periodicity and trends of the entire multi-node photovoltaic system
Dynamic Graph Construction: Real-time adaptive generation of adjacency matrices based on node embedding similarity
Spatiotemporal Fusion: Interactive learning framework combining spatial and temporal modules
Superior Performance: Outperforms state-of-the-art methods on multiple benchmark datasets
Strong Generalizability: Applicable to various time series forecasting domains beyond photovoltaic systems

ğŸš€ Quick Start
Installation
bash# Clone the repository
git clone https://github.com/username/G_GraphTST.git
cd G_GraphTST

# Create virtual environment
conda create -n g_graphtst python=3.8
conda activate g_graphtst

# Install dependencies
pip install -r requirements.txt
Basic Usage
bash# Run G-GraphTST on Solar dataset
python run.py --model GraphPatchTST --data solar --seq_len 96 --pred_len 96 --batch_size 16

# Run on ETT dataset with custom parameters
python run.py --model GraphPatchTST --data ETTh1 --seq_len 96 --pred_len 96 --d_model 512 --n_heads 8
ğŸ“ Project Structure
G_GraphTST/
â”‚
â”œâ”€â”€ ğŸ“‚ data/                    # Datasets storage
â”‚   â”œâ”€â”€ solar.csv              # Solar power dataset
â”‚   â”œâ”€â”€ ETTh1.csv              # Electricity Transformer Temperature dataset
â”‚   â””â”€â”€ ...                    # Other benchmark datasets
â”‚
â”œâ”€â”€ ğŸ“‚ results/                 # Experimental results
â”‚   â”œâ”€â”€ loss.npy              # Training loss curves (3D array: [samples, pred_len, channels])
â”‚   â”œâ”€â”€ metrics.npy           # Performance metrics (MAE, MSE, RMSE, MAPE, MSPE, RÂ²)
â”‚   â””â”€â”€ ...                    # Model-specific results
â”‚
â”œâ”€â”€ ğŸ“‚ checkpoints/            # Saved model checkpoints
â”‚   â””â”€â”€ model_best.pth        # Best performing model weights
â”‚
â”œâ”€â”€ ğŸ“‚ test_results/           # Prediction visualization (auto-generated)
â”‚   â”œâ”€â”€ prediction_curves.pdf # Prediction vs ground truth plots
â”‚   â””â”€â”€ ...                    # Additional visualization files
â”‚
â”œâ”€â”€ ğŸ“‚ models/                 # Model implementations
â”‚   â”œâ”€â”€ GraphPatchTST.py      # Main G-GraphTST model
â”‚   â”œâ”€â”€ baselines/            # Baseline model implementations
â”‚   â””â”€â”€ ...                    # Other model variants
â”‚
â”œâ”€â”€ ğŸ“‚ layers/                 # Neural network layer components
â”‚   â”œâ”€â”€ Transformer_EncDec.py # Transformer encoder-decoder layers
â”‚   â”œâ”€â”€ SelfAttention_Family.py # Multi-head attention mechanisms
â”‚   â”œâ”€â”€ Embed.py              # Embedding layers (patch, positional, temporal)
â”‚   â””â”€â”€ ...                    # Additional layer implementations
â”‚
â”œâ”€â”€ ğŸ“‚ utils/                  # Utility functions
â”‚   â”œâ”€â”€ tools.py              # General utilities (early stopping, learning rate adjustment)
â”‚   â”œâ”€â”€ metrics.py            # Evaluation metrics calculation
â”‚   â”œâ”€â”€ timefeatures.py       # Time feature engineering
â”‚   â””â”€â”€ ...                    # Other helper functions
â”‚
â”œâ”€â”€ ğŸ“‚ exp/                    # Experiment workflow
â”‚   â”œâ”€â”€ exp_basic.py          # Base experiment class
â”‚   â”œâ”€â”€ exp_long_term_forecasting.py # Long-term forecasting experiments
â”‚   â””â”€â”€ ...                    # Other experiment types
â”‚
â”œâ”€â”€ ğŸ“„ run.py                  # Main program entry point
â”œâ”€â”€ ğŸ“„ requirements.txt       # Python dependencies
â”œâ”€â”€ ğŸ“„ README.md              # This file
â””â”€â”€ ğŸ“„ LICENSE                # License information
ğŸ”§ Data Processing Pipeline
1. Data Preprocessing
Our preprocessing pipeline ensures data compatibility and optimal model performance:

Data Cleaning: Remove inconsistencies and handle missing values
Normalization: Apply statistical normalization for stable training
Dataset Splitting: Chronological split (70% train, 10% validation, 20% test)

2. Sliding Window Operation
Time series forecasting relies on sliding window preprocessing:
python# Example sliding window configuration
seq_len = 96      # Input sequence length (96 time steps)
pred_len = 96     # Prediction sequence length (96 time steps) 
stride = 1        # Sliding window stride
For detailed sliding window principles, refer to: Time Series Sliding Window Guide
3. Data Loading
Processed data flows through PyTorch DataLoader for efficient batch processing and model training.
